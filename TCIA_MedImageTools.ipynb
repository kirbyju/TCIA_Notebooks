{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Med-ImageTools\n",
    "This notebook is designed to showcase the core functionalities of Med-ImageTools and help you get started with the package. The notebook will guide you through the following steps:\n",
    "\n",
    "1. Installing the package\n",
    "2. Processing a sample TCIA dataset using `AutoPipeline` for deep learning segmentation\n",
    "   \n",
    "   i. Understanding outputs from `AutoPipeline` for segmentation\n",
    "   \n",
    "   ii. Understanding full outputs from `AutoPipeline`\n",
    "   \n",
    "3. *(Optional) Processing a sample TCIA dataset using `AutoPipeline` with radiotherapy data*\n",
    "\n",
    "## 1. Installing the package\n",
    "\n",
    "Med-ImageTools is available on PyPI and can be installed using pip:\n",
    "\n",
    "```\n",
    "pip install med-imagetools\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install med-imagetools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import imgtools\n",
    "    print(\"Looks like Med-ImageTools is installed!\")\n",
    "except ImportError as e:\n",
    "    print(e, \"Please install the imgtools package\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing a sample package with the AutoPipeline feature\n",
    "\n",
    "We're going to start off by defining where the dataset is located, and where you want the processed outputs to be saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH  =\"/path/to/tcia/dataset\"\n",
    "OUTPUT_PATH =\"/path/to/output/folder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. AutoPipeline dry-run\n",
    "Now let's dry-run AutoPipeline to understand it's crawl functionality.  \n",
    "We'll use the same command, but add the **--dry-run** flag to see what it would do without actually running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!autopipeline \\\n",
    "     $INPUT_PATH \\\n",
    "     $OUTPUT_PATH \\\n",
    "     --modalities CT,RTSTRUCT \\\n",
    "     --n_jobs 4 \\\n",
    "     --dry_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `AutoPipeline` creates a `.imgtools` folder in the dataset's parent directory.\n",
    "\n",
    "```\n",
    "parent_folder\n",
    "└───.imgtools\n",
    "│   ├── imgtools_dataset.csv\n",
    "│   ├── imgtools_dataset.json\n",
    "│   └── imgtools_dataset_edges.csv\n",
    "│ \n",
    "└───dataset\n",
    "    ├── patient-001\n",
    "    ├── patient-002\n",
    "    ...\n",
    "```\n",
    " \n",
    "There are three files in the `.imgtools` folder:\n",
    "\n",
    "* `imgtools_dataset.csv` contains the metadata for the dataset\n",
    "* `imgtools_dataset.json` contains the metadata for the dataset in JSON format\n",
    "* `imgtools_dataset_edges.csv` contains the \"edges\" for the dataset. \n",
    "    * An edge is a DICOM-DICOM pair that are connected based on the metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "parent_folder   = os.path.dirname(INPUT_PATH)\n",
    "imgtools_folder = os.path.join(parent_folder, \".imgtools\")\n",
    "imgtools_files  = os.listdir(imgtools_folder)\n",
    "\n",
    "print(\"Files generated by Med-ImageTools:\\n\")\n",
    "print(\"\\n\".join(imgtools_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the crawled dataset looks like. \n",
    "Each row represents a DICOM series (CT, MRI, PET, RTSTRUCT, SEG, RTDOSE, RTPLAN, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crawl = pd.read_csv(os.path.join(imgtools_folder, imgtools_files[0]), index_col=0)\n",
    "df_crawl.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the adjoined edges of the dataset looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = pd.read_csv(os.path.join(imgtools_folder, imgtools_files[-1]))\n",
    "df_edges.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many edges of each type we have in this dataset.\n",
    "There are 8 edge types detected by Med-ImageTools:\n",
    "* (0) RTDOSE-RTSTRUCT\n",
    "* (1) RTDOSE-CT\n",
    "* (2) RTSTRUCT-CT\n",
    "* (3) RTSTRUCT-PET\n",
    "* (4) CT-PET\n",
    "* (5) RTDOSE-PET\n",
    "* (6) RTPLAN-RTSTRUCT\n",
    "* (7) SEG-CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_edges.edge_type.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. AutoPipeline FULL run\n",
    "Now let's actually run the AutoPipeline and see what we get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!autopipeline \\\n",
    "     $INPUT_PATH \\\n",
    "     $OUTPUT_PATH \\\n",
    "     --modalities CT,RTSTRUCT \\\n",
    "     --n_jobs 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output folder will be structured like this:\n",
    "```\n",
    "output_folder\n",
    "├── dataset.csv\n",
    "├── report.md\n",
    "│ \n",
    "├── 0_patient-001\n",
    "│   ├── CT\n",
    "│   │   └── CT.nii.gz\n",
    "│   └── RTSTRUCT_CT\n",
    "│       ├── Head.nii.gz\n",
    "│       ├── Shoulder.nii.gz\n",
    "│       ├── Knees.nii.gz\n",
    "│       └── Toes.nii.gz\n",
    "│ \n",
    "├── 1_patient-002\n",
    "├── 2_patient-003\n",
    "...\n",
    "```\n",
    "\n",
    "Let's see what's inside the folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folders = [path for path in os.listdir(OUTPUT_PATH) if os.path.isdir(os.path.join(OUTPUT_PATH, path))]\n",
    "print(\"Output folders:\\n\")\n",
    "print(\"\\n\".join(output_folders))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the `dataset.csv` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(os.path.join(OUTPUT_PATH, \"dataset.csv\"), index_col=0)\n",
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 main types of columns in the dataset.csv file that are important for analysis:\n",
    "* `patient_ID`: Defines the patient ID excluding index number. \n",
    "* `output_folder_{modality}`: Path to the output folder per modality. \n",
    "  * For example, for CT,RTSTRUCT modality pairs, the output folders will be `output_folder_CT` and `output_folder_RTSTRUCT_CT`. \n",
    "* DICOM imaging metadata: Imaging parameters saved in the metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to create a PyTorch Dataset/DataLoader using a Med-ImageTools processed dataset, you can use the `dataset.csv` to easily refer to the data.\n",
    "\n",
    "Here's an example of what a PyTorch Dataset/DataLoader might look like:\n",
    "\n",
    "```python\n",
    "class MedImageToolsDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_folder, \n",
    "                 roi=\"GTV\"):\n",
    "        self.data_dir = data_folder\n",
    "        self.data_df  = pd.read_csv(os.path.join(data_folder, \"dataset.csv\"))\n",
    "        self.roi      = roi\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the row of the dataframe\n",
    "        row = self.data_df.iloc[idx]\n",
    "        \n",
    "        # get image and mask\n",
    "        img = sitk.ReadImage(os.path.join(self.data_dir, row[\"output_folder_CT\"], \"CT.nii.gz\"))\n",
    "        mask = sitk.ReadImage(os.path.join(self.data_dir, row[\"output_folder_RTSTRUCT_CT\"], f\"{self.roi}.nii.gz\"))\n",
    "        \n",
    "        # return the pair!\n",
    "        return img, mask\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(MedImageToolsDataset(dataset), batch_size=32)\n",
    "```\n",
    "\n",
    "A less-simplified version of the code with safer error handling and more comments looks like this. Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import SimpleITK as sitk\n",
    "import re\n",
    "import pathlib\n",
    "\n",
    "# Define the Dataset class\n",
    "class MedImageToolsDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_folder, \n",
    "                 roi=\"GTV\"):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_folder : str\n",
    "            Path to the folder containing the dataset.csv file and the output folders\n",
    "        roi : str\n",
    "            Name of the Region of Interest (ROI) to extract from the RTSTRUCT masks. \n",
    "            Regex expressions are accepted.\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(data_folder):\n",
    "            raise FileNotFoundError(f\"Folder {data_folder} does not exist\")\n",
    "        self.data_dir = data_folder\n",
    "\n",
    "        # Load the dataset.csv file\n",
    "        data_df_path = os.path.join(data_folder, \"dataset.csv\")\n",
    "        if not os.path.exists(data_df_path):\n",
    "            raise FileNotFoundError(f\"File dataset.csv not found in {data_folder}\")\n",
    "        self.data_df  = pd.read_csv(data_df_path)\n",
    "\n",
    "        self.output_cols   = [col for col in self.data_df.columns if col.startswith(\"output_folder_\")]\n",
    "        self.roi           = roi\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_df.iloc[idx]\n",
    "\n",
    "        for col in self.output_cols:\n",
    "            if 'folder_CT' in col:\n",
    "                img_path = pathlib.Path(self.data_dir, row[col], \"CT.nii.gz\").as_posix()\n",
    "                if os.path.exists(img_path):\n",
    "                    img = sitk.ReadImage(img_path)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"CT image not found at {img_path}\")\n",
    "                    break\n",
    "            elif 'RTSTRUCT' in col:\n",
    "                mask_folder_path = pathlib.Path(self.data_dir, row[col]).as_posix()\n",
    "                if os.path.exists(mask_folder_path):\n",
    "                    for mask_file in os.listdir(mask_folder_path):\n",
    "                        roi_name = mask_file.split(\".\")[0]\n",
    "                        if re.fullmatch(self.roi, roi_name, flags=re.IGNORECASE) or self.roi in roi_name:\n",
    "                            mask = sitk.ReadImage(os.path.join(self.data_dir, row[col], mask_file))\n",
    "                            break\n",
    "                else:\n",
    "                    continue\n",
    "                if 'mask' not in locals():\n",
    "                    raise FileNotFoundError(f\"Mask of {self.roi} not found in {row[col]}\")\n",
    "        \n",
    "        if 'img' in locals() and 'mask' in locals():\n",
    "            return img, mask\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "# Define a collate function\n",
    "def my_collate(batch):\n",
    "    \"Puts each data field into a tensor with outer dimension batch size\"\n",
    "    return [x for x in batch if x is not None]\n",
    "    # batch = filter(lambda x: x is not None, batch)\n",
    "    # return default_collate(batch)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(MedImageToolsDataset(OUTPUT_PATH, \n",
    "                                             roi=\"LUNG_L\"), \n",
    "                        batch_size=4, \n",
    "                        collate_fn=my_collate)\n",
    "\n",
    "# Print the first batch of data\n",
    "batch = next(iter(dataloader))\n",
    "print(\"Batch size:\", len(batch))\n",
    "\n",
    "img, mask = batch[0]\n",
    "print(f\"Image: {img.GetSize()}\")\n",
    "print(f\"Mask: {mask.GetSize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
